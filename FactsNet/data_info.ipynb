{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_testdata_path():\n",
    "    test_data_path = \"/crawl/crawler/test_data\"\n",
    "    test_data_path_list = []\n",
    "\n",
    "    for cat in os.listdir(test_data_path):\n",
    "        sub_path = os.path.join(test_data_path,cat)\n",
    "\n",
    "        for cat_2 in os.listdir(sub_path):\n",
    "            edge_path = os.path.join(sub_path, cat_2)\n",
    "            test_data_path_list.append(edge_path)\n",
    "            \n",
    "    return test_data_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path_list = load_testdata_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['general', 'history', 'lifestyle', 'nature', 'science', 'world']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(query_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = \"/crawler/query_output\"\n",
    "query_path_list = []\n",
    "\n",
    "\n",
    "for cat in os.listdir(query_path):\n",
    "    if cat == 'general':\n",
    "        continue\n",
    "    else:\n",
    "        sub_path = os.path.join(query_path,cat)\n",
    "        # print(cat, ' ', len(os.listdir(sub_path)))\n",
    "        for cat_2 in os.listdir(sub_path):\n",
    "            edge_path = os.path.join(sub_path, cat_2)\n",
    "            print(cat, ' ', len(os.listdir(edge_path)))\n",
    "            print(edge_path)\n",
    "            for cat_3 in os.listdir(edge_path):\n",
    "                file_path = os.path.join(edge_path, cat_3)\n",
    "                query_path_list.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = []\n",
    "lifestyle_path = []\n",
    "nature_path = []\n",
    "science_path = []\n",
    "world_path = []\n",
    "\n",
    "for q in test_data_path_list:\n",
    "    if q.split('/')[8] == 'history':\n",
    "        history_path.append(q)\n",
    "    elif q.split('/')[8] == 'lifestyle':\n",
    "        lifestyle_path.append(q)\n",
    "    elif q.split('/')[8] == 'nature':\n",
    "        nature_path.append(q)\n",
    "    elif q.split('/')[8] == 'science':\n",
    "        science_path.append(q)\n",
    "    elif q.split('/')[8] == 'world':\n",
    "        world_path.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "simcse_ckpt = 'princeton-nlp/sup-simcse-bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(simcse_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_length_data_num(test_data_path_list):\n",
    "    token_length = []\n",
    "    data_num = []\n",
    "    for test_data_path in test_data_path_list:\n",
    "        test_data = pd.read_csv(test_data_path)\n",
    "        data_num.append(test_data.shape[0])\n",
    "        for t in list(test_data.text):\n",
    "            token_length.append(len(tokenizer.encode(t)) - 2)\n",
    "\n",
    "    return token_length, data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_token_length, history_data_num = tok_length_data_num(history_path)\n",
    "lifestyle_token_length, lifestyle_data_num = tok_length_data_num(lifestyle_path)\n",
    "nature_token_length, nature_data_num = tok_length_data_num(nature_path)\n",
    "science_token_length, science_data_num = tok_length_data_num(science_path)\n",
    "world_token_length, world_data_num = tok_length_data_num(world_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_summary(token_legnth, data_num):\n",
    "    print(\"Token_length: \", np.average(token_legnth))\n",
    "    print('Data_num: ', np.average(data_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total version \n",
      "\n",
      "Token_length:  44.0908606353493\n",
      "Data_num:  193.41580756013747\n"
     ]
    }
   ],
   "source": [
    "print('total version \\n')\n",
    "print_summary(history_token_length+lifestyle_token_length+nature_token_length+science_token_length+world_token_length, \\\n",
    "              history_data_num+lifestyle_data_num+nature_data_num+science_data_num+world_data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token_length:  45.254056958624396\n",
      "Data_num:  166.16071428571428\n"
     ]
    }
   ],
   "source": [
    "print_summary(history_token_length, history_data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token_length:  39.38177874186551\n",
      "Data_num:  220.47826086956522\n"
     ]
    }
   ],
   "source": [
    "print_summary(lifestyle_token_length, lifestyle_data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token_length:  43.48164335664335\n",
      "Data_num:  169.4814814814815\n"
     ]
    }
   ],
   "source": [
    "print_summary(nature_token_length, nature_data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token_length:  43.99084716900809\n",
      "Data_num:  213.54545454545453\n"
     ]
    }
   ],
   "source": [
    "print_summary(science_token_length, science_data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token_length:  45.96067342410928\n",
      "Data_num:  203.42477876106196\n"
     ]
    }
   ],
   "source": [
    "print_summary(world_token_length, world_data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keonwoo_neo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473a17fb9c005091cf4353665de455d9c74d3d37a8dc30122c5c3420cae1d5ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
