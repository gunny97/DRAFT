{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/keonwoo/anaconda3/envs/KoDiffCSE/sroberta_change_lr were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'discriminator.embeddings.position_ids', 'discriminator.embeddings.word_embeddings.weight', 'discriminator.embeddings.position_embeddings.weight', 'discriminator.embeddings.token_type_embeddings.weight', 'discriminator.embeddings.LayerNorm.weight', 'discriminator.embeddings.LayerNorm.bias', 'discriminator.encoder.layer.0.attention.self.query.weight', 'discriminator.encoder.layer.0.attention.self.query.bias', 'discriminator.encoder.layer.0.attention.self.key.weight', 'discriminator.encoder.layer.0.attention.self.key.bias', 'discriminator.encoder.layer.0.attention.self.value.weight', 'discriminator.encoder.layer.0.attention.self.value.bias', 'discriminator.encoder.layer.0.attention.output.dense.weight', 'discriminator.encoder.layer.0.attention.output.dense.bias', 'discriminator.encoder.layer.0.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.0.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.0.intermediate.dense.weight', 'discriminator.encoder.layer.0.intermediate.dense.bias', 'discriminator.encoder.layer.0.output.dense.weight', 'discriminator.encoder.layer.0.output.dense.bias', 'discriminator.encoder.layer.0.output.LayerNorm.weight', 'discriminator.encoder.layer.0.output.LayerNorm.bias', 'discriminator.encoder.layer.1.attention.self.query.weight', 'discriminator.encoder.layer.1.attention.self.query.bias', 'discriminator.encoder.layer.1.attention.self.key.weight', 'discriminator.encoder.layer.1.attention.self.key.bias', 'discriminator.encoder.layer.1.attention.self.value.weight', 'discriminator.encoder.layer.1.attention.self.value.bias', 'discriminator.encoder.layer.1.attention.output.dense.weight', 'discriminator.encoder.layer.1.attention.output.dense.bias', 'discriminator.encoder.layer.1.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.1.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.1.intermediate.dense.weight', 'discriminator.encoder.layer.1.intermediate.dense.bias', 'discriminator.encoder.layer.1.output.dense.weight', 'discriminator.encoder.layer.1.output.dense.bias', 'discriminator.encoder.layer.1.output.LayerNorm.weight', 'discriminator.encoder.layer.1.output.LayerNorm.bias', 'discriminator.encoder.layer.2.attention.self.query.weight', 'discriminator.encoder.layer.2.attention.self.query.bias', 'discriminator.encoder.layer.2.attention.self.key.weight', 'discriminator.encoder.layer.2.attention.self.key.bias', 'discriminator.encoder.layer.2.attention.self.value.weight', 'discriminator.encoder.layer.2.attention.self.value.bias', 'discriminator.encoder.layer.2.attention.output.dense.weight', 'discriminator.encoder.layer.2.attention.output.dense.bias', 'discriminator.encoder.layer.2.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.2.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.2.intermediate.dense.weight', 'discriminator.encoder.layer.2.intermediate.dense.bias', 'discriminator.encoder.layer.2.output.dense.weight', 'discriminator.encoder.layer.2.output.dense.bias', 'discriminator.encoder.layer.2.output.LayerNorm.weight', 'discriminator.encoder.layer.2.output.LayerNorm.bias', 'discriminator.encoder.layer.3.attention.self.query.weight', 'discriminator.encoder.layer.3.attention.self.query.bias', 'discriminator.encoder.layer.3.attention.self.key.weight', 'discriminator.encoder.layer.3.attention.self.key.bias', 'discriminator.encoder.layer.3.attention.self.value.weight', 'discriminator.encoder.layer.3.attention.self.value.bias', 'discriminator.encoder.layer.3.attention.output.dense.weight', 'discriminator.encoder.layer.3.attention.output.dense.bias', 'discriminator.encoder.layer.3.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.3.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.3.intermediate.dense.weight', 'discriminator.encoder.layer.3.intermediate.dense.bias', 'discriminator.encoder.layer.3.output.dense.weight', 'discriminator.encoder.layer.3.output.dense.bias', 'discriminator.encoder.layer.3.output.LayerNorm.weight', 'discriminator.encoder.layer.3.output.LayerNorm.bias', 'discriminator.encoder.layer.4.attention.self.query.weight', 'discriminator.encoder.layer.4.attention.self.query.bias', 'discriminator.encoder.layer.4.attention.self.key.weight', 'discriminator.encoder.layer.4.attention.self.key.bias', 'discriminator.encoder.layer.4.attention.self.value.weight', 'discriminator.encoder.layer.4.attention.self.value.bias', 'discriminator.encoder.layer.4.attention.output.dense.weight', 'discriminator.encoder.layer.4.attention.output.dense.bias', 'discriminator.encoder.layer.4.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.4.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.4.intermediate.dense.weight', 'discriminator.encoder.layer.4.intermediate.dense.bias', 'discriminator.encoder.layer.4.output.dense.weight', 'discriminator.encoder.layer.4.output.dense.bias', 'discriminator.encoder.layer.4.output.LayerNorm.weight', 'discriminator.encoder.layer.4.output.LayerNorm.bias', 'discriminator.encoder.layer.5.attention.self.query.weight', 'discriminator.encoder.layer.5.attention.self.query.bias', 'discriminator.encoder.layer.5.attention.self.key.weight', 'discriminator.encoder.layer.5.attention.self.key.bias', 'discriminator.encoder.layer.5.attention.self.value.weight', 'discriminator.encoder.layer.5.attention.self.value.bias', 'discriminator.encoder.layer.5.attention.output.dense.weight', 'discriminator.encoder.layer.5.attention.output.dense.bias', 'discriminator.encoder.layer.5.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.5.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.5.intermediate.dense.weight', 'discriminator.encoder.layer.5.intermediate.dense.bias', 'discriminator.encoder.layer.5.output.dense.weight', 'discriminator.encoder.layer.5.output.dense.bias', 'discriminator.encoder.layer.5.output.LayerNorm.weight', 'discriminator.encoder.layer.5.output.LayerNorm.bias', 'discriminator.encoder.layer.6.attention.self.query.weight', 'discriminator.encoder.layer.6.attention.self.query.bias', 'discriminator.encoder.layer.6.attention.self.key.weight', 'discriminator.encoder.layer.6.attention.self.key.bias', 'discriminator.encoder.layer.6.attention.self.value.weight', 'discriminator.encoder.layer.6.attention.self.value.bias', 'discriminator.encoder.layer.6.attention.output.dense.weight', 'discriminator.encoder.layer.6.attention.output.dense.bias', 'discriminator.encoder.layer.6.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.6.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.6.intermediate.dense.weight', 'discriminator.encoder.layer.6.intermediate.dense.bias', 'discriminator.encoder.layer.6.output.dense.weight', 'discriminator.encoder.layer.6.output.dense.bias', 'discriminator.encoder.layer.6.output.LayerNorm.weight', 'discriminator.encoder.layer.6.output.LayerNorm.bias', 'discriminator.encoder.layer.7.attention.self.query.weight', 'discriminator.encoder.layer.7.attention.self.query.bias', 'discriminator.encoder.layer.7.attention.self.key.weight', 'discriminator.encoder.layer.7.attention.self.key.bias', 'discriminator.encoder.layer.7.attention.self.value.weight', 'discriminator.encoder.layer.7.attention.self.value.bias', 'discriminator.encoder.layer.7.attention.output.dense.weight', 'discriminator.encoder.layer.7.attention.output.dense.bias', 'discriminator.encoder.layer.7.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.7.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.7.intermediate.dense.weight', 'discriminator.encoder.layer.7.intermediate.dense.bias', 'discriminator.encoder.layer.7.output.dense.weight', 'discriminator.encoder.layer.7.output.dense.bias', 'discriminator.encoder.layer.7.output.LayerNorm.weight', 'discriminator.encoder.layer.7.output.LayerNorm.bias', 'discriminator.encoder.layer.8.attention.self.query.weight', 'discriminator.encoder.layer.8.attention.self.query.bias', 'discriminator.encoder.layer.8.attention.self.key.weight', 'discriminator.encoder.layer.8.attention.self.key.bias', 'discriminator.encoder.layer.8.attention.self.value.weight', 'discriminator.encoder.layer.8.attention.self.value.bias', 'discriminator.encoder.layer.8.attention.output.dense.weight', 'discriminator.encoder.layer.8.attention.output.dense.bias', 'discriminator.encoder.layer.8.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.8.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.8.intermediate.dense.weight', 'discriminator.encoder.layer.8.intermediate.dense.bias', 'discriminator.encoder.layer.8.output.dense.weight', 'discriminator.encoder.layer.8.output.dense.bias', 'discriminator.encoder.layer.8.output.LayerNorm.weight', 'discriminator.encoder.layer.8.output.LayerNorm.bias', 'discriminator.encoder.layer.9.attention.self.query.weight', 'discriminator.encoder.layer.9.attention.self.query.bias', 'discriminator.encoder.layer.9.attention.self.key.weight', 'discriminator.encoder.layer.9.attention.self.key.bias', 'discriminator.encoder.layer.9.attention.self.value.weight', 'discriminator.encoder.layer.9.attention.self.value.bias', 'discriminator.encoder.layer.9.attention.output.dense.weight', 'discriminator.encoder.layer.9.attention.output.dense.bias', 'discriminator.encoder.layer.9.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.9.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.9.intermediate.dense.weight', 'discriminator.encoder.layer.9.intermediate.dense.bias', 'discriminator.encoder.layer.9.output.dense.weight', 'discriminator.encoder.layer.9.output.dense.bias', 'discriminator.encoder.layer.9.output.LayerNorm.weight', 'discriminator.encoder.layer.9.output.LayerNorm.bias', 'discriminator.encoder.layer.10.attention.self.query.weight', 'discriminator.encoder.layer.10.attention.self.query.bias', 'discriminator.encoder.layer.10.attention.self.key.weight', 'discriminator.encoder.layer.10.attention.self.key.bias', 'discriminator.encoder.layer.10.attention.self.value.weight', 'discriminator.encoder.layer.10.attention.self.value.bias', 'discriminator.encoder.layer.10.attention.output.dense.weight', 'discriminator.encoder.layer.10.attention.output.dense.bias', 'discriminator.encoder.layer.10.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.10.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.10.intermediate.dense.weight', 'discriminator.encoder.layer.10.intermediate.dense.bias', 'discriminator.encoder.layer.10.output.dense.weight', 'discriminator.encoder.layer.10.output.dense.bias', 'discriminator.encoder.layer.10.output.LayerNorm.weight', 'discriminator.encoder.layer.10.output.LayerNorm.bias', 'discriminator.encoder.layer.11.attention.self.query.weight', 'discriminator.encoder.layer.11.attention.self.query.bias', 'discriminator.encoder.layer.11.attention.self.key.weight', 'discriminator.encoder.layer.11.attention.self.key.bias', 'discriminator.encoder.layer.11.attention.self.value.weight', 'discriminator.encoder.layer.11.attention.self.value.bias', 'discriminator.encoder.layer.11.attention.output.dense.weight', 'discriminator.encoder.layer.11.attention.output.dense.bias', 'discriminator.encoder.layer.11.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.11.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.11.intermediate.dense.weight', 'discriminator.encoder.layer.11.intermediate.dense.bias', 'discriminator.encoder.layer.11.output.dense.weight', 'discriminator.encoder.layer.11.output.dense.bias', 'discriminator.encoder.layer.11.output.LayerNorm.weight', 'discriminator.encoder.layer.11.output.LayerNorm.bias', 'mlp.net.0.weight', 'mlp.net.1.weight', 'mlp.net.1.bias', 'mlp.net.1.running_mean', 'mlp.net.1.running_var', 'mlp.net.1.num_batches_tracked', 'mlp.net.3.weight', 'mlp.net.4.running_mean', 'mlp.net.4.running_var', 'mlp.net.4.num_batches_tracked', 'generator.roberta.embeddings.position_ids', 'generator.roberta.embeddings.word_embeddings.weight', 'generator.roberta.embeddings.position_embeddings.weight', 'generator.roberta.embeddings.token_type_embeddings.weight', 'generator.roberta.embeddings.LayerNorm.weight', 'generator.roberta.embeddings.LayerNorm.bias', 'generator.roberta.encoder.layer.0.attention.self.query.weight', 'generator.roberta.encoder.layer.0.attention.self.query.bias', 'generator.roberta.encoder.layer.0.attention.self.key.weight', 'generator.roberta.encoder.layer.0.attention.self.key.bias', 'generator.roberta.encoder.layer.0.attention.self.value.weight', 'generator.roberta.encoder.layer.0.attention.self.value.bias', 'generator.roberta.encoder.layer.0.attention.output.dense.weight', 'generator.roberta.encoder.layer.0.attention.output.dense.bias', 'generator.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.0.intermediate.dense.weight', 'generator.roberta.encoder.layer.0.intermediate.dense.bias', 'generator.roberta.encoder.layer.0.output.dense.weight', 'generator.roberta.encoder.layer.0.output.dense.bias', 'generator.roberta.encoder.layer.0.output.LayerNorm.weight', 'generator.roberta.encoder.layer.0.output.LayerNorm.bias', 'generator.roberta.encoder.layer.1.attention.self.query.weight', 'generator.roberta.encoder.layer.1.attention.self.query.bias', 'generator.roberta.encoder.layer.1.attention.self.key.weight', 'generator.roberta.encoder.layer.1.attention.self.key.bias', 'generator.roberta.encoder.layer.1.attention.self.value.weight', 'generator.roberta.encoder.layer.1.attention.self.value.bias', 'generator.roberta.encoder.layer.1.attention.output.dense.weight', 'generator.roberta.encoder.layer.1.attention.output.dense.bias', 'generator.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.1.intermediate.dense.weight', 'generator.roberta.encoder.layer.1.intermediate.dense.bias', 'generator.roberta.encoder.layer.1.output.dense.weight', 'generator.roberta.encoder.layer.1.output.dense.bias', 'generator.roberta.encoder.layer.1.output.LayerNorm.weight', 'generator.roberta.encoder.layer.1.output.LayerNorm.bias', 'generator.roberta.encoder.layer.2.attention.self.query.weight', 'generator.roberta.encoder.layer.2.attention.self.query.bias', 'generator.roberta.encoder.layer.2.attention.self.key.weight', 'generator.roberta.encoder.layer.2.attention.self.key.bias', 'generator.roberta.encoder.layer.2.attention.self.value.weight', 'generator.roberta.encoder.layer.2.attention.self.value.bias', 'generator.roberta.encoder.layer.2.attention.output.dense.weight', 'generator.roberta.encoder.layer.2.attention.output.dense.bias', 'generator.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.2.intermediate.dense.weight', 'generator.roberta.encoder.layer.2.intermediate.dense.bias', 'generator.roberta.encoder.layer.2.output.dense.weight', 'generator.roberta.encoder.layer.2.output.dense.bias', 'generator.roberta.encoder.layer.2.output.LayerNorm.weight', 'generator.roberta.encoder.layer.2.output.LayerNorm.bias', 'generator.roberta.encoder.layer.3.attention.self.query.weight', 'generator.roberta.encoder.layer.3.attention.self.query.bias', 'generator.roberta.encoder.layer.3.attention.self.key.weight', 'generator.roberta.encoder.layer.3.attention.self.key.bias', 'generator.roberta.encoder.layer.3.attention.self.value.weight', 'generator.roberta.encoder.layer.3.attention.self.value.bias', 'generator.roberta.encoder.layer.3.attention.output.dense.weight', 'generator.roberta.encoder.layer.3.attention.output.dense.bias', 'generator.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.3.intermediate.dense.weight', 'generator.roberta.encoder.layer.3.intermediate.dense.bias', 'generator.roberta.encoder.layer.3.output.dense.weight', 'generator.roberta.encoder.layer.3.output.dense.bias', 'generator.roberta.encoder.layer.3.output.LayerNorm.weight', 'generator.roberta.encoder.layer.3.output.LayerNorm.bias', 'generator.roberta.encoder.layer.4.attention.self.query.weight', 'generator.roberta.encoder.layer.4.attention.self.query.bias', 'generator.roberta.encoder.layer.4.attention.self.key.weight', 'generator.roberta.encoder.layer.4.attention.self.key.bias', 'generator.roberta.encoder.layer.4.attention.self.value.weight', 'generator.roberta.encoder.layer.4.attention.self.value.bias', 'generator.roberta.encoder.layer.4.attention.output.dense.weight', 'generator.roberta.encoder.layer.4.attention.output.dense.bias', 'generator.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.4.intermediate.dense.weight', 'generator.roberta.encoder.layer.4.intermediate.dense.bias', 'generator.roberta.encoder.layer.4.output.dense.weight', 'generator.roberta.encoder.layer.4.output.dense.bias', 'generator.roberta.encoder.layer.4.output.LayerNorm.weight', 'generator.roberta.encoder.layer.4.output.LayerNorm.bias', 'generator.roberta.encoder.layer.5.attention.self.query.weight', 'generator.roberta.encoder.layer.5.attention.self.query.bias', 'generator.roberta.encoder.layer.5.attention.self.key.weight', 'generator.roberta.encoder.layer.5.attention.self.key.bias', 'generator.roberta.encoder.layer.5.attention.self.value.weight', 'generator.roberta.encoder.layer.5.attention.self.value.bias', 'generator.roberta.encoder.layer.5.attention.output.dense.weight', 'generator.roberta.encoder.layer.5.attention.output.dense.bias', 'generator.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.5.intermediate.dense.weight', 'generator.roberta.encoder.layer.5.intermediate.dense.bias', 'generator.roberta.encoder.layer.5.output.dense.weight', 'generator.roberta.encoder.layer.5.output.dense.bias', 'generator.roberta.encoder.layer.5.output.LayerNorm.weight', 'generator.roberta.encoder.layer.5.output.LayerNorm.bias', 'generator.lm_head.bias', 'generator.lm_head.dense.weight', 'generator.lm_head.dense.bias', 'generator.lm_head.layer_norm.weight', 'generator.lm_head.layer_norm.bias', 'generator.lm_head.decoder.weight', 'generator.lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /home/keonwoo/anaconda3/envs/KoDiffCSE/sroberta_change_lr and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "\n",
    "ckpt_path = \"/home/keonwoo/anaconda3/envs/KoDiffCSE/sroberta_change_lr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path, num_labels=9)\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contentDataset(Dataset):\n",
    "    def __init__(self, file, tok, max_len, pad_index=None):\n",
    "        super().__init__()\n",
    "        self.tok =tok\n",
    "        self.max_len = max_len\n",
    "        self.content = pd.read_csv(file)\n",
    "        self.len = self.content.shape[0]\n",
    "        self.pad_index = self.tok.pad_token\n",
    "    \n",
    "    def add_padding_data(self, inputs, max_len):\n",
    "        if len(inputs) < max_len:\n",
    "            # pad = np.array([self.pad_index] * (max_len - len(inputs)))\n",
    "            pad = np.array([0] * (max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "            return inputs\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "            return inputs\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        instance = self.content.iloc[idx]\n",
    "        # text = \"[CLS]\" + instance['content'] + \"[SEP]\"\n",
    "        text = instance['text']\n",
    "        input_ids = self.tok.encode(text)\n",
    "        \n",
    "        input_ids = self.add_padding_data(input_ids, max_len=self.max_len)\n",
    "        label_ids = instance['label']\n",
    "        # encoder_attention_mask = input_ids.ne(0).float()\n",
    "        return {\"encoder_input_ids\" : np.array(input_ids, dtype=np.int_),\n",
    "                \"label\" : np.array(label_ids,dtype=np.int_)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/keonwoo/anaconda3/envs/bgmRS/data/labeled_data_0706.csv')\n",
    "dataset.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "dataset['label'] = pd.factorize(dataset['label'])[0]\n",
    "# dataset.columns = ['label','text']\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_num = int(len(dataset)*0.9)\n",
    "trainset = dataset.iloc[:train_num]\n",
    "validset = dataset.iloc[train_num:]\n",
    "\n",
    "trainset.to_csv('/home/keonwoo/anaconda3/envs/bgmRS/data/trainset.csv')\n",
    "validset.to_csv('/home/keonwoo/anaconda3/envs/bgmRS/data/validset.csv')\n",
    "\n",
    "train_setup = contentDataset(file = \"/home/keonwoo/anaconda3/envs/bgmRS/data/trainset.csv\",tok = tokenizer, max_len = 128)\n",
    "valid_setup = contentDataset(file = \"/home/keonwoo/anaconda3/envs/bgmRS/data/validset.csv\",tok = tokenizer, max_len = 128)\n",
    "\n",
    "\n",
    "tarin_dataloader = DataLoader(train_setup, batch_size=256, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_setup, batch_size=256, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(tarin_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3a7947abf84a85a97ae9472fe15767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "loss_list = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tarin_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        encoder_attention_mask = batch[\"encoder_input_ids\"].ne(0).float().to(device)\n",
    "        outputs = model(batch['encoder_input_ids'], attention_mask=encoder_attention_mask, labels=batch['label'])\n",
    "        loss = outputs.loss\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(loss_list)\n",
    "plt.title('train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/home/keonwoo/anaconda3/envs/bgmRS/ckpt/koDiffCSE_0802\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/home/keonwoo/anaconda3/envs/bgmRS/ckpt/koDiffCSE_0802\", num_labels=9)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric\n",
    "\n",
    "pred = []\n",
    "ref = []\n",
    "\n",
    "model.eval()\n",
    "for batch in valid_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    encoder_attention_mask = batch[\"encoder_input_ids\"].ne(0).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch['encoder_input_ids'], attention_mask=encoder_attention_mask, labels=batch['label'])\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    pred.append(predictions)\n",
    "    ref.append(batch['label'])\n",
    "\n",
    "pred = torch.cat(pred, 0)\n",
    "ref = torch.cat(ref, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6332335329341318}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = load_metric(\"accuracy\")\n",
    "recall = load_metric(\"recall\")\n",
    "f1 = load_metric(\"f1\")\n",
    "prec = load_metric(\"precision\")\n",
    "\n",
    "\n",
    "acc.compute(predictions=pred, references=ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6346931984942193}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.6332335329341318}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.6320196245185103}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('KoDiffCSE': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4c199caf109837f69b68a7f5cf9c98cdd88a078cc35e48eb39a3d0f9b5eb33c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
