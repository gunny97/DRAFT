{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at jhgan/ko-sbert-sts and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "\n",
    "ckpt_path = \"jhgan/ko-sbert-sts\"\n",
    "sbert_tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "sbert_model = AutoModelForSequenceClassification.from_pretrained(ckpt_path, num_labels=9)\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contentDataset(Dataset):\n",
    "    def __init__(self, file, tok, max_len, pad_index=None):\n",
    "        super().__init__()\n",
    "        self.tok =tok\n",
    "        self.max_len = max_len\n",
    "        self.content = pd.read_csv(file)\n",
    "        self.len = self.content.shape[0]\n",
    "        self.pad_index = self.tok.pad_token\n",
    "    \n",
    "    def add_padding_data(self, inputs, max_len):\n",
    "        if len(inputs) < max_len:\n",
    "            # pad = np.array([self.pad_index] * (max_len - len(inputs)))\n",
    "            pad = np.array([0] * (max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "            return inputs\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "            return inputs\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        instance = self.content.iloc[idx]\n",
    "        # text = \"[CLS]\" + instance['content'] + \"[SEP]\"\n",
    "        text = instance['text']\n",
    "        input_ids = self.tok.encode(text)\n",
    "        \n",
    "        input_ids = self.add_padding_data(input_ids, max_len=self.max_len)\n",
    "        label_ids = instance['label']\n",
    "        # encoder_attention_mask = input_ids.ne(0).float()\n",
    "        return {\"encoder_input_ids\" : np.array(input_ids, dtype=np.int_),\n",
    "                \"label\" : np.array(label_ids,dtype=np.int_)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/keonwoo/anaconda3/envs/bgmRS/data/labeled_data_0706.csv')\n",
    "dataset.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "dataset['label'] = pd.factorize(dataset['label'])[0]\n",
    "# dataset.columns = ['label','text']\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_num = int(len(dataset)*0.9)\n",
    "trainset = dataset.iloc[:train_num]\n",
    "validset = dataset.iloc[train_num:]\n",
    "\n",
    "trainset.to_csv('/home/keonwoo/anaconda3/envs/bgmRS/data/trainset.csv')\n",
    "validset.to_csv('/home/keonwoo/anaconda3/envs/bgmRS/data/validset.csv')\n",
    "\n",
    "train_setup = contentDataset(file = \"/home/keonwoo/anaconda3/envs/bgmRS/data/trainset.csv\",tok = sbert_tokenizer, max_len = 512)\n",
    "valid_setup = contentDataset(file = \"/home/keonwoo/anaconda3/envs/bgmRS/data/validset.csv\",tok = sbert_tokenizer, max_len = 512)\n",
    "\n",
    "\n",
    "tarin_dataloader = DataLoader(train_setup, batch_size=32, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_setup, batch_size=32, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(sbert_model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(tarin_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:2\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "sbert_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/940 [00:23<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "loss_list = []\n",
    "\n",
    "sbert_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tarin_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        encoder_attention_mask = batch[\"encoder_input_ids\"].ne(0).float().to(device)\n",
    "        outputs = sbert_model(batch['encoder_input_ids'], attention_mask=encoder_attention_mask, labels=batch['label'])\n",
    "        loss = outputs.loss\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(loss_list)\n",
    "plt.title('train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model.save_pretrained(\"/home/keonwoo/anaconda3/envs/bgmRS/ckpt/sbert_0706\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/home/keonwoo/anaconda3/envs/bgmRS/ckpt/sbert_0706\", num_labels=9)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric\n",
    "\n",
    "pred = []\n",
    "ref = []\n",
    "\n",
    "model.eval()\n",
    "for batch in valid_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    encoder_attention_mask = batch[\"encoder_input_ids\"].ne(0).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch['encoder_input_ids'], attention_mask=encoder_attention_mask, labels=batch['label'])\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    pred.append(predictions)\n",
    "    ref.append(batch['label'])\n",
    "\n",
    "pred = torch.cat(pred, 0)\n",
    "ref = torch.cat(ref, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6002994011976048}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = load_metric(\"accuracy\")\n",
    "recall = load_metric(\"recall\")\n",
    "f1 = load_metric(\"f1\")\n",
    "prec = load_metric(\"precision\")\n",
    "\n",
    "\n",
    "acc.compute(predictions=pred, references=ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6205042316771254}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.6002994011976048}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.6056715600424968}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41957/2334514741.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validset['pred'] = pred.detach().cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "validset['pred'] = pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41957/1985121609.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validset['Actual Label'] = validset['label'].apply(lambda x: change_label(x))\n",
      "/tmp/ipykernel_41957/1985121609.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validset['Pred Label'] = validset['pred'].apply(lambda x: change_label(x))\n",
      "/tmp/ipykernel_41957/1985121609.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validset.drop(['label','pred'],axis=1,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "label_dict = {\n",
    "    '희망적인' : 0,  # 소분류\n",
    "    '감동적인' : 1, # 소분류\n",
    "    '무서운' : 2,  # 불안 -> 소분류\n",
    "    '로맨틱한' : 3, # 소분류\n",
    "    '평화로운' : 4, # 소분류\n",
    "    '멋진' : 5, # 소분류\n",
    "    '우스운' : 6, # 소분류\n",
    "    '신나는' : 7, # 소분류\n",
    "    '쿨한' : 8 # 소분류\n",
    "}\n",
    "\n",
    "reversed_dict = dict(map(reversed, label_dict.items()))\n",
    "\n",
    "def change_label(x):\n",
    "    return reversed_dict[x]\n",
    "\n",
    "validset['Actual Label'] = validset['label'].apply(lambda x: change_label(x))\n",
    "validset['Pred Label'] = validset['pred'].apply(lambda x: change_label(x))\n",
    "validset.drop(['label','pred'],axis=1,inplace=True)\n",
    "\n",
    "validset.to_csv('tmp_snu_added_0706__bad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Actual Label</th>\n",
       "      <th>Pred Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>딸 대학 발표를 기다리며 긴장이 되는군요.</td>\n",
       "      <td>쿨한</td>\n",
       "      <td>쿨한</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>애인과 부모님과의 관계 모두 잘 이루어 나가시길 바라요.</td>\n",
       "      <td>희망적인</td>\n",
       "      <td>희망적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아빠랑 있을 때 편안한 기분이 드시겠어요. 다른 좋은 점도 있으신가요?</td>\n",
       "      <td>로맨틱한</td>\n",
       "      <td>감동적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>집에 들어가기 싫을 무슨 일이 있으셨나요?</td>\n",
       "      <td>무서운</td>\n",
       "      <td>우스운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>오랜 작업에 대한 습관보다 배운다는 생각으로 업무를 해서 잘 적응했으면 좋겠어요.</td>\n",
       "      <td>로맨틱한</td>\n",
       "      <td>로맨틱한</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text Actual Label Pred Label\n",
       "0                        딸 대학 발표를 기다리며 긴장이 되는군요.           쿨한         쿨한\n",
       "1                애인과 부모님과의 관계 모두 잘 이루어 나가시길 바라요.         희망적인       희망적인\n",
       "2        아빠랑 있을 때 편안한 기분이 드시겠어요. 다른 좋은 점도 있으신가요?         로맨틱한       감동적인\n",
       "3                        집에 들어가기 싫을 무슨 일이 있으셨나요?          무서운        우스운\n",
       "4  오랜 작업에 대한 습관보다 배운다는 생각으로 업무를 해서 잘 적응했으면 좋겠어요.         로맨틱한       로맨틱한"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('tmp_snu_added_0706__bad.csv')\n",
    "data.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_correct = []\n",
    "for i in range(len(data)):\n",
    "    if data.iloc[i]['Actual Label'] != data.iloc[i]['Pred Label']:\n",
    "        not_correct.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Actual Label</th>\n",
       "      <th>Pred Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>그 선배는 나를 이유 없이 항상 괴롭혔어. 그래도 이젠 해방되었어!</td>\n",
       "      <td>평화로운</td>\n",
       "      <td>희망적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>피로가 많이 쌓이셨군요.</td>\n",
       "      <td>감동적인</td>\n",
       "      <td>쿨한</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>앞으로 어떻게 하고 싶으신가요?</td>\n",
       "      <td>멋진</td>\n",
       "      <td>신나는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>막내가 사춘기라서 신경질적이었는데 요즘 들어서 좋아진 것 같아서 다행이야.</td>\n",
       "      <td>희망적인</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>항상 건강하시길 바랄게요.</td>\n",
       "      <td>멋진</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>또 한 해가 지나가겠지.</td>\n",
       "      <td>감동적인</td>\n",
       "      <td>로맨틱한</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>한편으론 앞으로 계속 좋은 인상을 유지하는 것은 배로 힘들 것 같아.</td>\n",
       "      <td>로맨틱한</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>시어머님을 모시고 가족들과 한 시간 거리 이내로라도 여행을 다녀와야지.</td>\n",
       "      <td>쿨한</td>\n",
       "      <td>감동적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>귀찮긴 하지만 행복하단 감정도 드시는군요</td>\n",
       "      <td>무서운</td>\n",
       "      <td>로맨틱한</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>앞으로도 밀리지 않고 저축하려면 어떻게 해야 할까요?</td>\n",
       "      <td>감동적인</td>\n",
       "      <td>신나는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>월세를 십만 원 덜 내게 되어서 안도감을 느끼시는 것 같아요.</td>\n",
       "      <td>멋진</td>\n",
       "      <td>희망적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>지금의 상황을 어떻게 극복할 수 있을까요?</td>\n",
       "      <td>감동적인</td>\n",
       "      <td>희망적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>같이 산책도 하고 푹 쉬어야지.</td>\n",
       "      <td>멋진</td>\n",
       "      <td>희망적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>지금의 기분이 더 잘 유지되길 바라요.</td>\n",
       "      <td>로맨틱한</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>많이 당황스러운 것 같은데 어떻게 해야 이 상황에서 벗어날 수 있을까요?</td>\n",
       "      <td>무서운</td>\n",
       "      <td>우스운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>플레이스테이션으로 여러 가지 게임을 함께 해.</td>\n",
       "      <td>로맨틱한</td>\n",
       "      <td>쿨한</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>좋은 컨디션으로 행운이 있기를 바라요.</td>\n",
       "      <td>감동적인</td>\n",
       "      <td>쿨한</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>그렇게 생각하시는 이유가 있으신가요?</td>\n",
       "      <td>신나는</td>\n",
       "      <td>무서운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>보인의 음식솜씨가 맘에 든다니 다행이네요.</td>\n",
       "      <td>신나는</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>작년부터 먹어왔던 한약의 효과가 나타나시는 게 기쁘시겠어요.</td>\n",
       "      <td>감동적인</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>아들이 술과 담배는 절대 하지 않기로 약속했으니 꼭 지킬 거야.</td>\n",
       "      <td>감동적인</td>\n",
       "      <td>신나는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>지금보다 일들이 더 많아질 테니 나와 뜻이 맞는 동업자를 찾아봐야겠어.</td>\n",
       "      <td>평화로운</td>\n",
       "      <td>감동적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>원하는 결과를 얻으셨으면 좋겠어요.</td>\n",
       "      <td>감동적인</td>\n",
       "      <td>신나는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>좋은 시간을 통해 아내분과의 관계가 더 돈독해지길 바라요.</td>\n",
       "      <td>평화로운</td>\n",
       "      <td>희망적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>응. 내심 걱정했거든.</td>\n",
       "      <td>희망적인</td>\n",
       "      <td>쿨한</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>이번 팀플레이는 팀원들이 적극적으로 임해서 크게 힘들지 않고 좋은 결과를 낼 수 있었어.</td>\n",
       "      <td>신나는</td>\n",
       "      <td>희망적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>우리 회사에서 나는 너무 중요한 존재 같아.</td>\n",
       "      <td>신나는</td>\n",
       "      <td>감동적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>요즘 많이 피곤하셨군요. 어떻게 하면 지금의 상황을 변화시킬 수 있을까요?</td>\n",
       "      <td>쿨한</td>\n",
       "      <td>감동적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>다들 나이 들어 보이기는 해도 아픈 데 없이 잘 지내고 있어서 든든해.</td>\n",
       "      <td>멋진</td>\n",
       "      <td>로맨틱한</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>유산과 여생에 대해 깊이 생각해 볼 거야.</td>\n",
       "      <td>우스운</td>\n",
       "      <td>감동적인</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text Actual Label Pred Label\n",
       "598              그 선배는 나를 이유 없이 항상 괴롭혔어. 그래도 이젠 해방되었어!         평화로운       희망적인\n",
       "601                                      피로가 많이 쌓이셨군요.         감동적인         쿨한\n",
       "602                                  앞으로 어떻게 하고 싶으신가요?           멋진        신나는\n",
       "603          막내가 사춘기라서 신경질적이었는데 요즘 들어서 좋아진 것 같아서 다행이야.         희망적인       평화로운\n",
       "605                                     항상 건강하시길 바랄게요.           멋진       평화로운\n",
       "606                                      또 한 해가 지나가겠지.         감동적인       로맨틱한\n",
       "608             한편으론 앞으로 계속 좋은 인상을 유지하는 것은 배로 힘들 것 같아.         로맨틱한       평화로운\n",
       "616            시어머님을 모시고 가족들과 한 시간 거리 이내로라도 여행을 다녀와야지.           쿨한       감동적인\n",
       "619                             귀찮긴 하지만 행복하단 감정도 드시는군요          무서운       로맨틱한\n",
       "620                      앞으로도 밀리지 않고 저축하려면 어떻게 해야 할까요?         감동적인        신나는\n",
       "621                 월세를 십만 원 덜 내게 되어서 안도감을 느끼시는 것 같아요.           멋진       희망적인\n",
       "622                            지금의 상황을 어떻게 극복할 수 있을까요?         감동적인       희망적인\n",
       "624                                  같이 산책도 하고 푹 쉬어야지.           멋진       희망적인\n",
       "625                              지금의 기분이 더 잘 유지되길 바라요.         로맨틱한       평화로운\n",
       "626           많이 당황스러운 것 같은데 어떻게 해야 이 상황에서 벗어날 수 있을까요?          무서운        우스운\n",
       "627                          플레이스테이션으로 여러 가지 게임을 함께 해.         로맨틱한         쿨한\n",
       "630                              좋은 컨디션으로 행운이 있기를 바라요.         감동적인         쿨한\n",
       "634                               그렇게 생각하시는 이유가 있으신가요?          신나는        무서운\n",
       "637                            보인의 음식솜씨가 맘에 든다니 다행이네요.          신나는       평화로운\n",
       "639                  작년부터 먹어왔던 한약의 효과가 나타나시는 게 기쁘시겠어요.         감동적인       평화로운\n",
       "641                아들이 술과 담배는 절대 하지 않기로 약속했으니 꼭 지킬 거야.         감동적인        신나는\n",
       "644            지금보다 일들이 더 많아질 테니 나와 뜻이 맞는 동업자를 찾아봐야겠어.         평화로운       감동적인\n",
       "647                                원하는 결과를 얻으셨으면 좋겠어요.         감동적인        신나는\n",
       "650                   좋은 시간을 통해 아내분과의 관계가 더 돈독해지길 바라요.         평화로운       희망적인\n",
       "653                                       응. 내심 걱정했거든.         희망적인         쿨한\n",
       "655  이번 팀플레이는 팀원들이 적극적으로 임해서 크게 힘들지 않고 좋은 결과를 낼 수 있었어.          신나는       희망적인\n",
       "656                           우리 회사에서 나는 너무 중요한 존재 같아.          신나는       감동적인\n",
       "658          요즘 많이 피곤하셨군요. 어떻게 하면 지금의 상황을 변화시킬 수 있을까요?           쿨한       감동적인\n",
       "660            다들 나이 들어 보이기는 해도 아픈 데 없이 잘 지내고 있어서 든든해.           멋진       로맨틱한\n",
       "663                            유산과 여생에 대해 깊이 생각해 볼 거야.          우스운       감동적인"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[not_correct].tail(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bgmRS': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97f12e04903685b25708d91ecd1d4aa07ab7b586436cb43ce5df299ed23dfd1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
