{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keonwoo/anaconda3/envs/bgmRS/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-SBERT-V40K-klueNLI-augSTS and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "\n",
    "ckpt_path = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'\n",
    "sbert_tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "sbert_model = AutoModelForSequenceClassification.from_pretrained(ckpt_path, num_labels=9)\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contentDataset(Dataset):\n",
    "    def __init__(self, file, tok, max_len, pad_index=None):\n",
    "        super().__init__()\n",
    "        self.tok =tok\n",
    "        self.max_len = max_len\n",
    "        self.content = pd.read_csv(file)\n",
    "        self.len = self.content.shape[0]\n",
    "        self.pad_index = self.tok.pad_token\n",
    "    \n",
    "    def add_padding_data(self, inputs, max_len):\n",
    "        if len(inputs) < max_len:\n",
    "            # pad = np.array([self.pad_index] * (max_len - len(inputs)))\n",
    "            pad = np.array([0] * (max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "            return inputs\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "            return inputs\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        instance = self.content.iloc[idx]\n",
    "        # text = \"[CLS]\" + instance['content'] + \"[SEP]\"\n",
    "        text = instance['text']\n",
    "        input_ids = self.tok.encode(text)\n",
    "        \n",
    "        input_ids = self.add_padding_data(input_ids, max_len=self.max_len)\n",
    "        label_ids = instance['label']\n",
    "        # encoder_attention_mask = input_ids.ne(0).float()\n",
    "        return {\"encoder_input_ids\" : np.array(input_ids, dtype=np.int_),\n",
    "                \"label\" : np.array(label_ids,dtype=np.int_)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/keonwoo/anaconda3/envs/bgmRS/data/labeled_data_0706.csv')\n",
    "dataset.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "dataset['label'] = pd.factorize(dataset['label'])[0]\n",
    "# dataset.columns = ['label','text']\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_num = int(len(dataset)*0.9)\n",
    "trainset = dataset.iloc[:train_num]\n",
    "validset = dataset.iloc[train_num:]\n",
    "\n",
    "trainset.to_csv('/home/keonwoo/anaconda3/envs/bgmRS/data/trainset.csv')\n",
    "validset.to_csv('/home/keonwoo/anaconda3/envs/bgmRS/data/validset.csv')\n",
    "\n",
    "train_setup = contentDataset(file = \"/home/keonwoo/anaconda3/envs/bgmRS/data/trainset.csv\",tok = sbert_tokenizer, max_len = 512)\n",
    "valid_setup = contentDataset(file = \"/home/keonwoo/anaconda3/envs/bgmRS/data/validset.csv\",tok = sbert_tokenizer, max_len = 512)\n",
    "\n",
    "\n",
    "tarin_dataloader = DataLoader(train_setup, batch_size=32, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_setup, batch_size=32, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(sbert_model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(tarin_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(40000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "sbert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 940/940 [3:10:42<00:00, 12.17s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "loss_list = []\n",
    "\n",
    "sbert_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tarin_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        encoder_attention_mask = batch[\"encoder_input_ids\"].ne(0).float().to(device)\n",
    "        outputs = sbert_model(batch['encoder_input_ids'], attention_mask=encoder_attention_mask, labels=batch['label'])\n",
    "        loss = outputs.loss\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(loss_list)\n",
    "plt.title('train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.makedirs(\"/home/keonwoo/anaconda3/envs/bgmRS/ckpt/sbert_snu\")\n",
    "\n",
    "sbert_model.save_pretrained(\"/home/keonwoo/anaconda3/envs/bgmRS/ckpt/sbert_snu_added_0706\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(40000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/home/keonwoo/anaconda3/envs/bgmRS/ckpt/sbert_snu_added_0706\", num_labels=9)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric\n",
    "\n",
    "pred = []\n",
    "ref = []\n",
    "\n",
    "model.eval()\n",
    "for batch in valid_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    encoder_attention_mask = batch[\"encoder_input_ids\"].ne(0).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch['encoder_input_ids'], attention_mask=encoder_attention_mask, labels=batch['label'])\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    pred.append(predictions)\n",
    "    ref.append(batch['label'])\n",
    "\n",
    "pred = torch.cat(pred, 0)\n",
    "ref = torch.cat(ref, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5853293413173652}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = load_metric(\"accuracy\")\n",
    "recall = load_metric(\"recall\")\n",
    "f1 = load_metric(\"f1\")\n",
    "prec = load_metric(\"precision\")\n",
    "\n",
    "\n",
    "acc.compute(predictions=pred, references=ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5873393452691532}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.5853293413173652}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.5846380774496138}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5523590333716916}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = load_metric(\"accuracy\")\n",
    "recall = load_metric(\"recall\")\n",
    "f1 = load_metric(\"f1\")\n",
    "prec = load_metric(\"precision\")\n",
    "\n",
    "\n",
    "acc.compute(predictions=pred, references=ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5570793837339553}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.5523590333716916}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.5532513359552833}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57634/2334514741.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validset['pred'] = pred.detach().cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "validset['pred'] = pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57634/402622896.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validset['Actual Label'] = validset['label'].apply(lambda x: change_label(x))\n",
      "/tmp/ipykernel_57634/402622896.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validset['Pred Label'] = validset['pred'].apply(lambda x: change_label(x))\n",
      "/tmp/ipykernel_57634/402622896.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validset.drop(['label','pred'],axis=1,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "label_dict = {\n",
    "    '희망적인' : 0,  # 소분류\n",
    "    '감동적인' : 1, # 소분류\n",
    "    '무서운' : 2,  # 불안 -> 소분류\n",
    "    '로맨틱한' : 3, # 소분류\n",
    "    '평화로운' : 4, # 소분류\n",
    "    '멋진' : 5, # 소분류\n",
    "    '우스운' : 6, # 소분류\n",
    "    '신나는' : 7, # 소분류\n",
    "    '쿨한' : 8 # 소분류\n",
    "}\n",
    "\n",
    "reversed_dict = dict(map(reversed, label_dict.items()))\n",
    "\n",
    "def change_label(x):\n",
    "    return reversed_dict[x]\n",
    "\n",
    "validset['Actual Label'] = validset['label'].apply(lambda x: change_label(x))\n",
    "validset['Pred Label'] = validset['pred'].apply(lambda x: change_label(x))\n",
    "validset.drop(['label','pred'],axis=1,inplace=True)\n",
    "\n",
    "validset.to_csv('tmp_snu_added_0706.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Actual Label</th>\n",
       "      <th>Pred Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>온돌 침대를 준 자식들에게 너무 고마워서 고마움의 표시를 하고 싶네.</td>\n",
       "      <td>감동적인</td>\n",
       "      <td>멋진</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어머니에게 도움이 되는 가장 좋은 방법이 어떤 방법일까요?</td>\n",
       "      <td>멋진</td>\n",
       "      <td>멋진</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>우리 부장님께서는 어려운 업무도 알기 쉽게 설명해 주셔서 너무 감사해.</td>\n",
       "      <td>감동적인</td>\n",
       "      <td>멋진</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>내 느낌에 이번 모의고사 평균 이 등급 받을 수 있을 거 같아!</td>\n",
       "      <td>신나는</td>\n",
       "      <td>신나는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>우리 둘 다 등산을 좋아해서 주말마다 아내랑 산에 다니는 것이 너무 만족스러워.</td>\n",
       "      <td>평화로운</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text Actual Label Pred Label\n",
       "0        온돌 침대를 준 자식들에게 너무 고마워서 고마움의 표시를 하고 싶네.         감동적인         멋진\n",
       "1              어머니에게 도움이 되는 가장 좋은 방법이 어떤 방법일까요?           멋진         멋진\n",
       "2       우리 부장님께서는 어려운 업무도 알기 쉽게 설명해 주셔서 너무 감사해.         감동적인         멋진\n",
       "3           내 느낌에 이번 모의고사 평균 이 등급 받을 수 있을 거 같아!          신나는        신나는\n",
       "4  우리 둘 다 등산을 좋아해서 주말마다 아내랑 산에 다니는 것이 너무 만족스러워.         평화로운       평화로운"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('tmp_snu_added_0706.csv')\n",
    "data.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_correct = []\n",
    "for i in range(len(data)):\n",
    "    if data.iloc[i]['Actual Label'] != data.iloc[i]['Pred Label']:\n",
    "        not_correct.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Actual Label</th>\n",
       "      <th>Pred Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>앞으로도 건강하셨으면 좋겠어요.</td>\n",
       "      <td>로맨틱한</td>\n",
       "      <td>신나는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>당신이 원하는 것을 다 할 수 있길 바라요.</td>\n",
       "      <td>로맨틱한</td>\n",
       "      <td>신나는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>내가 생각하던 나보다 실제의 나는 좀 더 별로인가 봐.</td>\n",
       "      <td>무서운</td>\n",
       "      <td>우스운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>응. 엄하기로 유명한 교수님인데 칭찬을 해주셨거든.</td>\n",
       "      <td>희망적인</td>\n",
       "      <td>멋진</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>친구와 함께 찍었던 사진이 위로가 되었으면 좋겠네요.</td>\n",
       "      <td>무서운</td>\n",
       "      <td>감동적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>무리하지 않는 선에서 일주일에 두세 번은 야외에서 운동을 해야겠어.</td>\n",
       "      <td>평화로운</td>\n",
       "      <td>로맨틱한</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>언니와 더욱 친해지면 좋겠네요.</td>\n",
       "      <td>멋진</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>상황을 극복하기 위해 어떤 행동을 하면 좋을까요?</td>\n",
       "      <td>감동적인</td>\n",
       "      <td>희망적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>기분이 좋으시군요.</td>\n",
       "      <td>쿨한</td>\n",
       "      <td>로맨틱한</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>무슨 좋은 일이 생겼어요?</td>\n",
       "      <td>쿨한</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>면접보고 오셨군요. 좋은 결과 기대해요!</td>\n",
       "      <td>신나는</td>\n",
       "      <td>희망적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>결혼 생활에 만족하고 계시는군요.</td>\n",
       "      <td>신나는</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>나의 투자담당자가 재산 관리를 지금처럼 앞으로도 잘할 거라고 믿고 있어.</td>\n",
       "      <td>감동적인</td>\n",
       "      <td>신나는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>많은 시행착오도 겪었지만 팀원들의 도움이 컸던 거 같아. 너무 고마웠어.</td>\n",
       "      <td>희망적인</td>\n",
       "      <td>멋진</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>다음에도 꼭 좋은 성적을 받아서 지금처럼 기쁘시면 좋겠어요.</td>\n",
       "      <td>신나는</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>만약 떨어지더라도 내용을 기억하고 있으니까 또 시험 볼 거야.</td>\n",
       "      <td>신나는</td>\n",
       "      <td>희망적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>결과가 달라지지 않으니 가만히 있으려고 하시는군요.</td>\n",
       "      <td>무서운</td>\n",
       "      <td>감동적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>나도 그런 상황이 닥치면 망설이지 않고 도와주려고 해.</td>\n",
       "      <td>멋진</td>\n",
       "      <td>희망적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>딸이 힘들까 봐 걱정했는데 다행히 금방 낳았나 봐. 딸도 손녀도 다 건강하다고 하네.</td>\n",
       "      <td>평화로운</td>\n",
       "      <td>희망적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>그런 선생님께 솔직하게 말씀 드리고 내가 잘못한 점들을 반성하고 싶어.</td>\n",
       "      <td>멋진</td>\n",
       "      <td>우스운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>일요일만 아니면 모든 요일이 다 괜찮아. 일요일은 쉬어야지.</td>\n",
       "      <td>쿨한</td>\n",
       "      <td>로맨틱한</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>요즘 머리가 자주 아파서 검진을 받았더니 뇌종양이라고 하네. 세상이 무너지는 기분이야.</td>\n",
       "      <td>감동적인</td>\n",
       "      <td>무서운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>친구들과 함께 즐거운 주말 보내시길 바라요.</td>\n",
       "      <td>평화로운</td>\n",
       "      <td>감동적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>아내가 퇴원하는 날 병원에서 돈 가방을 주워서 경찰서에 맡겼는데 오늘 사례금을 준다...</td>\n",
       "      <td>쿨한</td>\n",
       "      <td>희망적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>응 맞아. 엄마와 함께하는 시간을 늘려야겠어.</td>\n",
       "      <td>멋진</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>앞으로 어떻게 하면 지금처럼 기쁜 마음을 유지할 수 있을까요?</td>\n",
       "      <td>신나는</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>그러셨군요. 그런데 드시기 힘드신가요?</td>\n",
       "      <td>무서운</td>\n",
       "      <td>멋진</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>제가 다 속상하네요. 건강 잃지 마세요.</td>\n",
       "      <td>무서운</td>\n",
       "      <td>감동적인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>합격을 해서 너무 기뻐!</td>\n",
       "      <td>쿨한</td>\n",
       "      <td>평화로운</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>내일도 열 시간 넘게 공부해야지. 의욕이 넘쳐.</td>\n",
       "      <td>평화로운</td>\n",
       "      <td>쿨한</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text Actual Label Pred Label\n",
       "588                                  앞으로도 건강하셨으면 좋겠어요.         로맨틱한        신나는\n",
       "593                           당신이 원하는 것을 다 할 수 있길 바라요.         로맨틱한        신나는\n",
       "595                     내가 생각하던 나보다 실제의 나는 좀 더 별로인가 봐.          무서운        우스운\n",
       "596                       응. 엄하기로 유명한 교수님인데 칭찬을 해주셨거든.         희망적인         멋진\n",
       "600                      친구와 함께 찍었던 사진이 위로가 되었으면 좋겠네요.          무서운       감동적인\n",
       "601              무리하지 않는 선에서 일주일에 두세 번은 야외에서 운동을 해야겠어.         평화로운       로맨틱한\n",
       "602                                  언니와 더욱 친해지면 좋겠네요.           멋진       평화로운\n",
       "605                        상황을 극복하기 위해 어떤 행동을 하면 좋을까요?         감동적인       희망적인\n",
       "606                                         기분이 좋으시군요.           쿨한       로맨틱한\n",
       "608                                     무슨 좋은 일이 생겼어요?           쿨한       평화로운\n",
       "611                             면접보고 오셨군요. 좋은 결과 기대해요!          신나는       희망적인\n",
       "613                                 결혼 생활에 만족하고 계시는군요.          신나는       평화로운\n",
       "614           나의 투자담당자가 재산 관리를 지금처럼 앞으로도 잘할 거라고 믿고 있어.         감동적인        신나는\n",
       "621           많은 시행착오도 겪었지만 팀원들의 도움이 컸던 거 같아. 너무 고마웠어.         희망적인         멋진\n",
       "630                  다음에도 꼭 좋은 성적을 받아서 지금처럼 기쁘시면 좋겠어요.          신나는       평화로운\n",
       "632                 만약 떨어지더라도 내용을 기억하고 있으니까 또 시험 볼 거야.          신나는       희망적인\n",
       "633                       결과가 달라지지 않으니 가만히 있으려고 하시는군요.          무서운       감동적인\n",
       "636                     나도 그런 상황이 닥치면 망설이지 않고 도와주려고 해.           멋진       희망적인\n",
       "637    딸이 힘들까 봐 걱정했는데 다행히 금방 낳았나 봐. 딸도 손녀도 다 건강하다고 하네.         평화로운       희망적인\n",
       "639            그런 선생님께 솔직하게 말씀 드리고 내가 잘못한 점들을 반성하고 싶어.           멋진        우스운\n",
       "646                  일요일만 아니면 모든 요일이 다 괜찮아. 일요일은 쉬어야지.           쿨한       로맨틱한\n",
       "648   요즘 머리가 자주 아파서 검진을 받았더니 뇌종양이라고 하네. 세상이 무너지는 기분이야.         감동적인        무서운\n",
       "649                           친구들과 함께 즐거운 주말 보내시길 바라요.         평화로운       감동적인\n",
       "651  아내가 퇴원하는 날 병원에서 돈 가방을 주워서 경찰서에 맡겼는데 오늘 사례금을 준다...           쿨한       희망적인\n",
       "653                          응 맞아. 엄마와 함께하는 시간을 늘려야겠어.           멋진       평화로운\n",
       "654                 앞으로 어떻게 하면 지금처럼 기쁜 마음을 유지할 수 있을까요?          신나는       평화로운\n",
       "658                              그러셨군요. 그런데 드시기 힘드신가요?          무서운         멋진\n",
       "659                             제가 다 속상하네요. 건강 잃지 마세요.          무서운       감동적인\n",
       "661                                      합격을 해서 너무 기뻐!           쿨한       평화로운\n",
       "667                         내일도 열 시간 넘게 공부해야지. 의욕이 넘쳐.         평화로운         쿨한"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[not_correct].tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bgmRS': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97f12e04903685b25708d91ecd1d4aa07ab7b586436cb43ce5df299ed23dfd1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
