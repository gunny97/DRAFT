{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at BM-K/KoSimCSE-roberta were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at BM-K/KoSimCSE-roberta and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "\n",
    "ckpt_path = 'BM-K/KoSimCSE-roberta'\n",
    "sbert_tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "sbert_model = AutoModelForSequenceClassification.from_pretrained(ckpt_path, num_labels=9)\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contentDataset(Dataset):\n",
    "    def __init__(self, file, tok, max_len, pad_index=None):\n",
    "        super().__init__()\n",
    "        self.tok =tok\n",
    "        self.max_len = max_len\n",
    "        self.content = pd.read_csv(file)\n",
    "        self.len = self.content.shape[0]\n",
    "        self.pad_index = self.tok.pad_token\n",
    "    \n",
    "    def add_padding_data(self, inputs, max_len):\n",
    "        if len(inputs) < max_len:\n",
    "            # pad = np.array([self.pad_index] * (max_len - len(inputs)))\n",
    "            pad = np.array([0] * (max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "            return inputs\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "            return inputs\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        instance = self.content.iloc[idx]\n",
    "        # text = \"[CLS]\" + instance['content'] + \"[SEP]\"\n",
    "        text = instance['text']\n",
    "        input_ids = self.tok.encode(text)\n",
    "        \n",
    "        input_ids = self.add_padding_data(input_ids, max_len=self.max_len)\n",
    "        label_ids = instance['label']\n",
    "        # encoder_attention_mask = input_ids.ne(0).float()\n",
    "        return {\"encoder_input_ids\" : np.array(input_ids, dtype=np.int_),\n",
    "                \"label\" : np.array(label_ids,dtype=np.int_)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/keonwoo/anaconda3/envs/bgmRS/data/labeled_data_0706.csv')\n",
    "dataset.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "dataset['label'] = pd.factorize(dataset['label'])[0]\n",
    "# dataset.columns = ['label','text']\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_num = int(len(dataset)*0.9)\n",
    "trainset = dataset.iloc[:train_num]\n",
    "validset = dataset.iloc[train_num:]\n",
    "\n",
    "trainset.to_csv('/home/keonwoo/anaconda3/envs/bgmRS/data/trainset.csv')\n",
    "validset.to_csv('/home/keonwoo/anaconda3/envs/bgmRS/data/validset.csv')\n",
    "\n",
    "train_setup = contentDataset(file = \"/home/keonwoo/anaconda3/envs/bgmRS/data/trainset.csv\",tok = sbert_tokenizer, max_len = 128)\n",
    "valid_setup = contentDataset(file = \"/home/keonwoo/anaconda3/envs/bgmRS/data/validset.csv\",tok = sbert_tokenizer, max_len = 128)\n",
    "\n",
    "\n",
    "tarin_dataloader = DataLoader(train_setup, batch_size=256, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_setup, batch_size=256, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(sbert_model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(tarin_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "sbert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447ec7f782ba44c7a028066c5299b734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "loss_list = []\n",
    "\n",
    "sbert_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tarin_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        encoder_attention_mask = batch[\"encoder_input_ids\"].ne(0).float().to(device)\n",
    "        outputs = sbert_model(batch['encoder_input_ids'], attention_mask=encoder_attention_mask, labels=batch['label'])\n",
    "        loss = outputs.loss\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(loss_list)\n",
    "plt.title('train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.makedirs(\"/home/keonwoo/anaconda3/envs/bgmRS/ckpt/sbert_snu\")\n",
    "\n",
    "sbert_model.save_pretrained(\"/home/keonwoo/anaconda3/envs/bgmRS/ckpt/simcse_0707\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/home/keonwoo/anaconda3/envs/bgmRS/ckpt/simcse_0707\", num_labels=9)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric\n",
    "\n",
    "pred = []\n",
    "ref = []\n",
    "\n",
    "model.eval()\n",
    "for batch in valid_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    encoder_attention_mask = batch[\"encoder_input_ids\"].ne(0).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch['encoder_input_ids'], attention_mask=encoder_attention_mask, labels=batch['label'])\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    pred.append(predictions)\n",
    "    ref.append(batch['label'])\n",
    "\n",
    "pred = torch.cat(pred, 0)\n",
    "ref = torch.cat(ref, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.562874251497006}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = load_metric(\"accuracy\")\n",
    "recall = load_metric(\"recall\")\n",
    "f1 = load_metric(\"f1\")\n",
    "prec = load_metric(\"precision\")\n",
    "\n",
    "\n",
    "acc.compute(predictions=pred, references=ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5622220179433336}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.562874251497006}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.5563569609597907}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.compute(predictions=pred, references=ref, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('KoDiffCSE': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4c199caf109837f69b68a7f5cf9c98cdd88a078cc35e48eb39a3d0f9b5eb33c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
